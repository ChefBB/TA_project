\chapter*{Methods}
\label{ch:capitolo2}

% This section will provide an overview about the data, methods and procedures followed in the project.\\

The dataset used in this project is a sampled subset of English-language
songs derived from the \textit{Genius Song Lyrics Dataset}\textsuperscript{\cite{geniusdataset}}.
% The original dataset (3m records) included songs in many different languages;
% however, this work focused exclusively on English-language ones.
The original dataset contained numerous attributes; the ones considered
relevant for model training are:
\begin{itemize}
    \item \textbf{title:} the song's title;
    % \item \textbf{artist:} the artist
    % \item \textbf{year:}
    % \item \textbf{views:}
    % \item \textbf{features:}

    % \item \textbf{id:}
    \item \textbf{lemmatized\_stanzas:} lyrics of the single stanza;
    
    \item \textbf{stanza\_number:} identifies the position of the stanza in the song;

    \item \textbf{is\_chorus:} boolean variable that attests whether the stanza is
        a chorus or not;
    
    \item \textbf{is\_country, is\_pop, is\_rap, is\_rb, is\_rock:} boolean variables, result of a one-hot encoding process, that represent songs genres;

    \item \textbf{label:} represents the emotional classification of the stanza,
        assigned by Albert Base v2\textsuperscript{\cite{albert-base-v2}} model.
    
    % \item \textbf{is\_country:}
    % \item \textbf{is\_pop:}
    % \item \textbf{is\_rap:}
    % \item \textbf{is\_rb:}
    % \item \textbf{is\_rock:}
\end{itemize}

% The dataset originally didn't contain emotion labels, essential for training the models.
% To create the ground truth, the model
% Albert Base v2\textsuperscript{\cite{albert-base-v2}} was used, classifying
% stanzas' lyrics into Plutchik's eight primary emotions.
All of these attributes, except for \texttt{title}, were the result
of the preprocessing phase, as described in section~\ref{preprocessing}.
Due to limited computational power, the labeling process was time-intensive,
ultimately resulting in a limited dataset, with a few more than 100.000 entries.

%PREPROCESSING
\subsection*{Preprocessing}
\label{preprocessing}
The initial preprocessing step involved sampling from the original dataset
while maintaining the proportional distribution of genres.
This approach ensured that the genre representation in the sampled subset
accurately reflected that of the full dataset.\\

The preliminary text cleaning process focused on the \texttt{lyrics} attribute,
which contained the complete lyrics of each song in string format.
Initially, a regular expression (RegEx) was built to remove noise from the
lyrics, specifically targeting words enclosed in square brackets that were
irrelevant to the stanza splitting process. Many keywords marking different
stanzas were written within square brackets, and removing non-keyword items
inside brackets was crucial to avoid potential issues.\\

The next critical step was stanza splitting. After cleaning texts from
noisy square-bracketed items, lyrics were split based on various keywords
used to denote stanzas (such as "chorus", "verse", "intro", "outro", "refrain", "hook", etc.).
The RegEx developed accounted for the different formats in which these keywords
appeared, including square brackets, parentheses, or no brackets at all, as well
as stanzas separated only by double newline characters.
The output of this step was, for each song record, a list of strings
representing individual stanzas (each stanza has also a header with the corresponding
keyword; this aspect will be discussed in the next paragraph).
Next, uninformative strings—such as empty strings or those with fewer
than 20 characters—were removed, as they were too short to provide meaningful
content.
As a result, the output of this preliminary preprocessing phase was a dataset
where the records were no longer whole songs but individual stanzas, each
numbered according to its position within the song.\\

A further and more detailed cleaning process on the stanzas involved the creation
of the boolean feature \texttt{is\_chorus}, which was assigned a \texttt{true}
value for repeated stanzas within the same song or for stanzas with headers such
as "hook", "chorus", "refrain", or "bridge".
Next, stanza headers and newline characters between verses were removed to obtain
cleaner stanzas.
Since choruses, hooks, bridges, and refrains often repeat throughout songs,
duplicate stanzas were discarded to avoid redundant data. This resulted in a
dataset of cleaned, non-duplicate stanzas, which served as the checkpoint for
the labeling step and the starting point for the text lemmatization process.\\

The subsequent step involved lemmatizing the stanzas using the \texttt{spaCy}
library. A list of lemmatized tokens was created by filtering out punctuation
and empty words. Lemmatization was chosen over stemming because it produces
more accurate and meaningful results, particularly for tasks requiring semantic
understanding, such as the one at hand.\\

Since the dataset was not pre-labeled at the stanza level, ALBERT Base v2 was employed for this task.
This transformer model is specifically designed to be fine-tuned on tasks that
require an understanding of the entire sentence, such as sequence classification.



%MODELLI STATICI
\subsection*{Static Models}
Firstly, it is important to illustrate the feature creation step applied in order
to enhance model performance.
This step involved generating five features for each emotion class using Term
Frequency-Inverse Document Frequency (TF-IDF), a statistical measure that
evaluates how informative and important a word is within a document or class.
In this project, TF-IDF was applied to identify the five most informative words
for characterizing each emotion.
The \texttt{min\_df} and \texttt{max\_df} parameters were employed during the
TF-IDF computation to establish minimum and maximum thresholds.
Specifically, a word was considered informative for a class if it appeared at
least twice and in up to the 80\% of the stanzas associated with that class.
These features quantify the number of times that certain word appears for each
class.
This step was incorporated into the project's pipeline because the initial model
training, that will be illustrated in the following sections, revealed suboptimal
classification performance. 
The addition of these features led to a slight improvement in the results.
\textbf{E' EFFETTIVAMENTE VERO?}\\

The development of static models was then simple and straight forward.
Static models were mainly developed to provide a performance comparison
for the more complex Neural Networks.
The two architectures are the same, consisting of a preprocessing
layer to handle the inputs, followed by the classifier itself.\\

The preprocessing layers handle both \texttt{title} and
\texttt{lemmatized\_stanzas} through TF-IDF. The boolean attributes
are converted to integers, while \texttt{stanza\_number} is scaled.\\

Random Search was chosen for hyperparameter tuning, for both models.
Cross validation is also used in order to provide a more accurate
estimate of model performance.

\subsection*{Neural Networks}
The architectures were developed and tuned through empirical,
reiterated testing. These parameters helped with the process, and can
be used for further experimentation.
Both the Recurrent Neural Network and the One-Dimensional Convolutional
Neural Network share the same preprocessing
architecture. Most attributes are processed in the same manner as in
the Static Models; specific steps are applied to \texttt{lemmatized\_stanzas}
and \texttt{title}.
Non-Negative Matrix Factorization is applied to \texttt{title} in addition to
Term Frequency-Inverse Document Frequency
to extract latent topics, providing a richer representation of the
textual data.\\

\texttt{lemmatized\_stanzas} are handled by Convolutional and Recurrent
pipelines of the two Networks.
Elements are first tokenized, and then padded in order to
get an input with consistent shape, which is essential for both types of
recurrent layers.\\

\subsubsection*{One-Dimensional Convolutional Neural Network}
The Convolutional part of the architecture is designed to extract and learn
local patterns in \texttt{embedding\_lyrics}.
Its structure consists of three convolutional layers, each applying filters of
varying sizes. This allows to detect patterns at different granularities.
These layers are followed by Global Max Pooling, to reduce the previous output's
dimension to a fixed-length vector, as well as retaining focus on the most
informative patterns.
A dropout layer is then applied, to introduce regularization and prevent
overfitting.


\subsubsection*{Recurrent Neural Network}
The Recurrent part of the architecture is designed to extract and learn
local patterns in \texttt{embedding\_lyrics}.
Its structure consists of three Gated Recurrent Units (\texttt{GRU} layers)
to model temporal relationships. These are characterized by progressively smaller
numbers of units; this allows pattern capture at different abstraction levels.
All three layers in the architecture use the \texttt{tanh} activation function
to compute the hidden state and
the \texttt{sigmoid} activation function for the recurrent gate.
The first and second layers return the full sequence of hidden states for each
time step in the input sequence, enabling richer learning of patterns over time.
Dropout is applied on every layer, to prevent overfitting and add regularization.


\subsection*{Shared Components}
The remaining features are handled by a simple pipeline, which concatenates
their Input layers.
The inputs are passed them through a dense layer to create a compact
representation.
The output of the lyrics-processing branch is concatenated with the processed
additional features.
The combined representation is passed through a Dense layer with 32 units
(ReLU activation), followed by a Dropout layer with a rate of 0.3.
Finally, the output layer uses 8 units with a softmax activation, corresponding
to the classification into 8 emotion categories.


\section*{Evaluation}
The performances of the models were evaluated using the
\texttt{classification\_report} function from the \texttt{scikit-learn} library. 
This function is particularly useful as it offers an overview of key evaluation
metrics commonly used in Machine Learning, i.e. accuracy, precision, recall,
and F1-score.\\
For the static models implemented in this project, the classification report
revealed an accuracy of 34\% for the Random Forest algorithm and 43\% for SVM. 
These results can be considered reasonable, given that the task at hand is a
multi-class classification problem with 8 classes.\\

\textbf{AGGIUNGERE ALTRO}