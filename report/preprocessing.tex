\chapter{Methods}
\label{ch:capitolo2}

This section will provide an overview about the data, methods and procedures used in the project.\\

The dataset used in this project represents a sampled subset of songs
derived from the Genius Song Lyrics Dataset\textsuperscript{\cite{geniusdataset}}.
The original dataset (3m records) included songs in many different languages; however, this work focused
exclusively on English-language ones.
The original dataset contained numerous attributes, but the ones considered relevant for model training are:
\begin{itemize}
    \item \textbf{title:} the song's title;
    % \item \textbf{artist:} the artist
    % \item \textbf{year:}
    % \item \textbf{views:}
    % \item \textbf{features:}

    % \item \textbf{id:}
    \item \textbf{lemmatized\_stanzas:} lyrics of the single stanza;
    
    \item \textbf{stanza\_number:} identifies the position of the stanza in the song;

    \item \textbf{is\_chorus:} boolean variable that attests whether the stanza is
        a chorus or not;
    
    \item \textbf{tag:} represents the genre of the song. For easier handling,
        this attribute of the original dataset has been one-hot encoded into various boolean variables
        (is\_country, is\_pop, is\_rap, is\_rb, is\_rock);

    \item \textbf{label:} represents the emotional classification of the stanza,
        assigned by Albert Base v2\textsuperscript{\cite{albert-base-v2}} model.
    
    % \item \textbf{is\_country:}
    % \item \textbf{is\_pop:}
    % \item \textbf{is\_rap:}
    % \item \textbf{is\_rb:}
    % \item \textbf{is\_rock:}
\end{itemize}

% The dataset originally didn't contain emotion labels, essential for training the models.
% To create the ground truth, the model
% Albert Base v2\textsuperscript{\cite{albert-base-v2}} was used, classifying
% stanzas' lyrics into Plutchik's eight primary emotions.
All of these attributes, except for the \textit{title} one, were the result of the preprocessing phase, as will be described later in the \textit{Preprocessing} paragraph.\\
Due to limited computational power, the labeling process was time-intensive, ultimately resulting in a limited dataset consisting of \textbf{(QUANTE? AGGIUNGEREI NUMERO STROFE)}.\\

The first step in the preprocessing phase of this dataset involved sampling from the original dataset while preserving the proportions of the different genres. 
This ensured that the genre distribution in the subset remained representative of the full dataset.\\
\\
The preliminary text cleaning process focused on the \textit{lyrics} attribute, which was the attribute of the original dataset that contained
the entire lyrics of each song (in string format). Initially, we built a RegEx to clean the lyrics' strings from noise, specifically targeting words enclosed between square brackets that were irrelevant for the stanza splitting process.
Many of the keywords marking different stanzas were written within square brackets, and removing the non-keyword items within brackets was essential to prevent potential issues. \\
The crucial step was the stanza splitting. After cleaning the strings from the noisy square-bracketed items, we split them based on various keywords used to denote stanzas (such as \textit{chorus, verse, intro, outro, refrain, hook} etc.). 
The RegEx we developed also accounted for the different formats in which these keywords appeared; between square brackets, parentheses, without brackets, only a double newline character between one stanza and the other.
The output of this step was, for each song record, a list of stings, corresponding to a list of stanzas (with the stanza's header as the corresponding keyword). \\
Next, we removed the resulting strings that were uninformative; such as empty strings or those with fewer than 20 characters, which were too short to provide useful content. \\
As a result, the output of this preliminary preprocessing phase is a dataset in which the records are not whole songs anymore but single stanzas; each numbered based on its position in the song. \\
\\
A further and deeper cleaning process on the stanzas involved the creation of the boolean feature \textit{is\_chorus}; \textit{true} value for repeated stanzas for the same song or stanzas that had \textit{hook, chorus, refrain, bridge} as a header. \\
We then removed the stanza headers and the newline characters between verses to obtain cleaner stanzas. \\ 
Since choruses, hooks, bridges and refrains often repeat throughout songs, we decided to drop duplicate stanzas in order to avoid redundant data.
This resulted in a dataset of cleaned and non-duplicate stanzas: the checkpoint for the labelling step and the starting point for the text lemmatization process. To label the dataset, the Albert Base v2 model has been used; this transformer model is primarily aimed at being fine-tuned on tasks that use the whole sentence to make decisions, such as sequence classification. 
\\
The next step involved lemmatizing stanzas using the \textit{spaCy} library. We created a list of lemmatized tokens (filtering punctuation and empty words). 
We opted for lemmatization over stemming because lemmatization produces more accurate and meaningful results, particularly for tasks requiring semantic understanding, such as in our case.








